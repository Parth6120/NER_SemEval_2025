{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity-Aware Machine Translation (EA-MT) with NER\n",
    "\n",
    "This notebook demonstrates a complete pipeline for entity-aware English-to-French machine translation using multi-task learning with NER. It integrates all code from the `src` folder, including data preparation, baseline translation, entity-aware translation, fine-tuning, and evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment Check\n",
    "Check for GPU availability and install required packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5fd93ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Device count: 1\n",
      "Current device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Device count:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "else:\n",
    "    print(\"No GPU detected. Training will use CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff567e37",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "Prepare the dataset for NER and translation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "917a2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure NLTK data is available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "def get_label_from_wikidata(qid):\n",
    "    \"\"\"Fetches the English label for a given Wikidata QID.\"\"\"\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        data = response.json()\n",
    "        return data['entities'][qid]['labels']['en']['value']\n",
    "    except (requests.exceptions.RequestException, KeyError, ValueError) as e:\n",
    "        print(f\"Could not fetch label for {qid}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_qid_to_label_mapping(df):\n",
    "    \"\"\"Creates a mapping from QID to its English label.\"\"\"\n",
    "    all_qids = set(qid for entity_list in df['entities'] for qid in entity_list)\n",
    "    qid_to_label = {qid: get_label_from_wikidata(qid) for qid in all_qids}\n",
    "    return qid_to_label\n",
    "\n",
    "def tokenize_and_iob(row, qid_to_label):\n",
    "    \"\"\"Tokenizes source text and creates IOB tags for entities.\"\"\"\n",
    "    text = row['source']\n",
    "    tokens = word_tokenize(text)\n",
    "    labels = ['O'] * len(tokens)\n",
    "\n",
    "    for qid in row['entities']:\n",
    "        entity_text = qid_to_label.get(qid)\n",
    "        if not entity_text:\n",
    "            continue\n",
    "        \n",
    "        entity_tokens = word_tokenize(entity_text)\n",
    "        if not entity_tokens:\n",
    "            continue\n",
    "\n",
    "        # Find entity in tokens and apply IOB tags\n",
    "        for i in range(len(tokens) - len(entity_tokens) + 1):\n",
    "            if tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
    "                labels[i] = 'B-ENT'\n",
    "                for j in range(1, len(entity_tokens)):\n",
    "                    labels[i+j] = 'I-ENT'\n",
    "                break  # Move to the next qid once tagged\n",
    "\n",
    "    return list(zip(tokens, labels))\n",
    "\n",
    "def prepare_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads data from a JSONL file and prepares it for NER and translation.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the .jsonl file.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with an added 'token_iob' column.\n",
    "    \"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_json(file_path, lines=True)\n",
    "    \n",
    "    print(\"Fetching entity labels from Wikidata...\")\n",
    "    qid_to_label = create_qid_to_label_mapping(df)\n",
    "    \n",
    "    print(\"Tokenizing and creating IOB tags...\")\n",
    "    df['token_iob'] = df.apply(lambda row: tokenize_and_iob(row, qid_to_label), axis=1)\n",
    "    \n",
    "    print(\"Data preparation complete.\")\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91026711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Fetching entity labels from Wikidata...\n",
      "Could not fetch label for Q23: 'en'\n",
      "Tokenizing and creating IOB tags...\n",
      "Data preparation complete.\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5531 entries, 0 to 5530\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   id             5531 non-null   object\n",
      " 1   source_locale  5531 non-null   object\n",
      " 2   target_locale  5531 non-null   object\n",
      " 3   source         5531 non-null   object\n",
      " 4   target         5531 non-null   object\n",
      " 5   entities       5531 non-null   object\n",
      " 6   from           5531 non-null   object\n",
      " 7   token_iob      5531 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 345.8+ KB\n",
      "\n",
      "First 5 rows of prepared data:\n",
      "         id source_locale target_locale  \\\n",
      "0  a9011ddf            en            fr   \n",
      "1  982450cf            en            fr   \n",
      "2  b218d184            en            fr   \n",
      "3  f477742c            en            fr   \n",
      "4  2e3cf745            en            fr   \n",
      "\n",
      "                                              source  \\\n",
      "0  What is the seventh tallest mountain in North ...   \n",
      "1           Who is the youngest current US governor?   \n",
      "2  Has Bernie Sanders ever been president of the ...   \n",
      "3  Which actor was Stephenie Meyer's first choice...   \n",
      "4  Which river is longer than the Mississippi River?   \n",
      "\n",
      "                                              target   entities     from  \\\n",
      "0  Quelle est la septième plus haute montagne d’A...      [Q49]  mintaka   \n",
      "1  Qui est l’actuel plus jeune gouverneur américa...  [Q889821]  mintaka   \n",
      "2  Bernie Sanders a-t-il déjà été Président des É...      [Q30]  mintaka   \n",
      "3  Quel acteur Stephanie Meyer a-t-elle choisi en...  [Q160219]  mintaka   \n",
      "4     Quel fleuve est plus long que le Mississippi ?    [Q1497]  mintaka   \n",
      "\n",
      "                                           token_iob  \n",
      "0  [(What, O), (is, O), (the, O), (seventh, O), (...  \n",
      "1  [(Who, O), (is, O), (the, O), (youngest, O), (...  \n",
      "2  [(Has, O), (Bernie, O), (Sanders, O), (ever, O...  \n",
      "3  [(Which, O), (actor, O), (was, O), (Stephenie,...  \n",
      "4  [(Which, O), (river, O), (is, O), (longer, O),...  \n",
      "\n",
      "Example of token_iob column:\n",
      "[('What', 'O'), ('is', 'O'), ('the', 'O'), ('seventh', 'O'), ('tallest', 'O'), ('mountain', 'O'), ('in', 'O'), ('North', 'B-ENT'), ('America', 'I-ENT'), ('?', 'O')]\n",
      "\n",
      "Prepared data saved to E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\prepared_data.csv\n"
     ]
    }
   ],
   "source": [
    "train_file = r'E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\Data\\references\\train\\fr\\train.jsonl'\n",
    "\n",
    "prepared_df = prepare_data(train_file)\n",
    "\n",
    "# Display info and head of the processed DataFrame\n",
    "print(\"\\nDataFrame Info:\")\n",
    "prepared_df.info()\n",
    "print(\"\\nFirst 5 rows of prepared data:\")\n",
    "print(prepared_df.head())\n",
    "print(\"\\nExample of token_iob column:\")\n",
    "print(prepared_df['token_iob'].iloc[0])\n",
    "\n",
    "# Save the prepared DataFrame as a CSV file in the data folder\n",
    "output_path = r'E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\prepared_data.csv'\n",
    "prepared_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nPrepared data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e4254",
   "metadata": {},
   "source": [
    "## 3. Baseline Translation\n",
    "Translate English sentences to French using a pre-trained MarianMT model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7bdf8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5531 rows from E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\prepared_data.csv\n",
      "Translating source sentences using Helsinki-NLP/opus-mt-en-fr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ptlpa\\anaconda3\\envs\\aisd_env1\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline translations saved to E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\baseline_translations.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import os\n",
    "\n",
    "def load_data(csv_path):\n",
    "    \"\"\"Loads the prepared CSV data.\"\"\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def translate_sentences(sentences, model_name=\"Helsinki-NLP/opus-mt-en-fr\", batch_size=8):\n",
    "    \"\"\"\n",
    "    Translates a list of English sentences to French using a pre-trained model.\n",
    "    Args:\n",
    "        sentences (list): List of English sentences.\n",
    "        model_name (str): Hugging Face model name.\n",
    "        batch_size (int): Number of sentences per batch.\n",
    "    Returns:\n",
    "        list: Translated French sentences.\n",
    "    \"\"\"\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    translations = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        translated = model.generate(**inputs)\n",
    "        outputs = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        translations.extend(outputs)\n",
    "    return translations\n",
    "\n",
    "# Set the path to your prepared data CSV (update this as needed)\n",
    "data_path = r\"E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\prepared_data.csv\"\n",
    "output_path = r\"E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\baseline_translations.csv\"\n",
    "\n",
    "# Load data\n",
    "df = load_data(data_path)\n",
    "print(f\"Loaded {len(df)} rows from {data_path}\")\n",
    "\n",
    "# Translate source sentences\n",
    "print(\"Translating source sentences using Helsinki-NLP/opus-mt-en-fr...\")\n",
    "df[\"mt_baseline\"] = translate_sentences(df[\"source\"].tolist())\n",
    "\n",
    "# Save the results\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Baseline translations saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930fb3e9",
   "metadata": {},
   "source": [
    "## 4. Entity-Aware Pipeline\n",
    "Inject placeholders for entities, translate, and post-process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29c36fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5531 rows from E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\prepared_data.csv\n",
      "Injecting placeholders for entities...\n",
      "Placeholder-injected data saved to E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\entity_placeholders.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def extract_entity_spans(token_iob):\n",
    "    \"\"\"Extracts spans for entities using IOB tags.\"\"\"\n",
    "    spans = []\n",
    "    current = None\n",
    "    for idx, (token, tag) in enumerate(token_iob):\n",
    "        if tag == 'B-ENT':\n",
    "            if current:\n",
    "                spans.append(current)\n",
    "            current = [idx, idx]\n",
    "        elif tag == 'I-ENT' and current:\n",
    "            current[1] = idx\n",
    "        else:\n",
    "            if current:\n",
    "                spans.append(current)\n",
    "                current = None\n",
    "    if current:\n",
    "        spans.append(current)\n",
    "    return spans\n",
    "\n",
    "def inject_placeholders(row):\n",
    "    \"\"\"Replaces entity spans with placeholders in the sentence.\"\"\"\n",
    "    token_iob = ast.literal_eval(row['token_iob'])\n",
    "    tokens = [tok for tok, tag in token_iob]\n",
    "    spans = extract_entity_spans(token_iob)\n",
    "    entities = ast.literal_eval(row['entities'])\n",
    "    placeholder_map = {}\n",
    "    new_tokens = tokens[:]\n",
    "    for idx, span in enumerate(spans):\n",
    "        placeholder = f\"@ENTITY{idx+1}@\"\n",
    "        # Replace entity tokens with placeholder\n",
    "        start, end = span\n",
    "        new_tokens[start:end+1] = [placeholder]\n",
    "        placeholder_map[placeholder] = entities[idx] if idx < len(entities) else None\n",
    "    row['placeholder_sentence'] = ' '.join(new_tokens)\n",
    "    row['placeholder_map'] = placeholder_map\n",
    "    return row\n",
    "\n",
    "# Set the path to your prepared data CSV (update this as needed)\n",
    "data_path = r\"E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\prepared_data.csv\"\n",
    "output_path = r\"E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\entity_placeholders.csv\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Loaded {len(df)} rows from {data_path}\")\n",
    "\n",
    "# Inject placeholders\n",
    "print(\"Injecting placeholders for entities...\")\n",
    "df = df.apply(inject_placeholders, axis=1)\n",
    "\n",
    "# Save the results\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Placeholder-injected data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cc3d4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5531 rows from E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\entity_placeholders.csv\n",
      "Translating placeholder-injected sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ptlpa\\anaconda3\\envs\\aisd_env1\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity-aware translations saved to E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\entity_placeholders_translated.csv\n"
     ]
    }
   ],
   "source": [
    "# Translate placeholder-injected sentences\n",
    "# df = pd.read_csv('entity_placeholders.csv')\n",
    "# df['mt_placeholder'] = translate_sentences(df['placeholder_sentence'].tolist())\n",
    "# df.to_csv('entity_placeholders_translated.csv', index=False)\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def load_data(csv_path):\n",
    "    \"\"\"Loads the placeholder-injected CSV data.\"\"\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def translate_sentences(sentences, model_name=\"Helsinki-NLP/opus-mt-en-fr\", batch_size=8):\n",
    "    \"\"\"\n",
    "    Translates a list of English sentences to French using a pre-trained model.\n",
    "    Args:\n",
    "        sentences (list): List of English sentences.\n",
    "        model_name (str): Hugging Face model name.\n",
    "        batch_size (int): Number of sentences per batch.\n",
    "    Returns:\n",
    "        list: Translated French sentences.\n",
    "    \"\"\"\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    translations = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        translated = model.generate(**inputs)\n",
    "        outputs = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        translations.extend(outputs)\n",
    "    return translations\n",
    "\n",
    "# Set the path to your placeholder-injected data CSV\n",
    "data_path = r\"E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\entity_placeholders.csv\"\n",
    "output_path = r\"E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\entity_placeholders_translated.csv\"\n",
    "\n",
    "# Load data\n",
    "df = load_data(data_path)\n",
    "print(f\"Loaded {len(df)} rows from {data_path}\")\n",
    "\n",
    "# Translate placeholder-injected sentences\n",
    "print(\"Translating placeholder-injected sentences...\")\n",
    "df[\"mt_placeholder\"] = translate_sentences(df[\"placeholder_sentence\"].tolist())\n",
    "\n",
    "# Save the results\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Entity-aware translations saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d00432d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5531 rows from E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\entity_placeholders_translated.csv\n",
      "Replacing placeholders with French entity labels...\n",
      "Could not fetch label for Q23: 'fr'\n",
      "Entity-aware translations saved to E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\entity_aware_translations.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import requests\n",
    "\n",
    "def get_label_from_wikidata(qid, lang='fr'):\n",
    "    \"\"\"Fetches the label for a given Wikidata QID in the specified language.\"\"\"\n",
    "    url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return data['entities'][qid]['labels'][lang]['value']\n",
    "    except Exception as e:\n",
    "        print(f\"Could not fetch label for {qid}: {e}\")\n",
    "        return None\n",
    "\n",
    "def replace_placeholders(row):\n",
    "    sentence = row['mt_placeholder']\n",
    "    placeholder_map = ast.literal_eval(row['placeholder_map']) if isinstance(row['placeholder_map'], str) else row['placeholder_map']\n",
    "    for placeholder, qid in placeholder_map.items():\n",
    "        if qid:\n",
    "            fr_label = get_label_from_wikidata(qid, lang='fr')\n",
    "            if not fr_label:\n",
    "                fr_label = qid  # fallback to QID if label not found\n",
    "            sentence = sentence.replace(placeholder, fr_label)\n",
    "    row['mt_entity_aware'] = sentence\n",
    "    return row\n",
    "\n",
    "# Set the path to your translated placeholder-injected CSV\n",
    "data_path = r\"E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\entity_placeholders_translated.csv\"\n",
    "output_path = r\"E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\entity_aware_translations.csv\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"Loaded {len(df)} rows from {data_path}\")\n",
    "\n",
    "# Replace placeholders with French entity labels\n",
    "print(\"Replacing placeholders with French entity labels...\")\n",
    "df = df.apply(replace_placeholders, axis=1)\n",
    "\n",
    "# Save the final entity-aware translations\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Entity-aware translations saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bd9067",
   "metadata": {},
   "source": [
    "## 5. Fine-tuning MarianMT (Optional)\n",
    "Fine-tune the translation model on placeholder-injected data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdccc3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ptlpa\\anaconda3\\envs\\aisd_env1\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be4cfa53073490e99f9cff4283e3c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ptlpa\\anaconda3\\envs\\aisd_env1\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3950: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ptlpa\\AppData\\Local\\Temp\\ipykernel_11080\\3722607209.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2076' max='2076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2076/2076 03:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.174800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.102700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.101000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.076100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.081700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.077900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ptlpa\\anaconda3\\envs\\aisd_env1\\lib\\site-packages\\transformers\\modeling_utils.py:3854: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved to E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\finetuned_placeholder_mt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "\n",
    "def load_data(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Use placeholder-injected English as source, and placeholder-injected French as target\n",
    "    # For training, you need to have both. If you don't have placeholder-injected French, use the original French with placeholders inserted at the same positions as in English.\n",
    "    # Here, we assume you have a column 'placeholder_sentence' (English) and 'target_placeholder' (French)\n",
    "    # If not, you may need to generate 'target_placeholder' first.\n",
    "    if 'target_placeholder' not in df.columns:\n",
    "        # Fallback: use 'target' (reference French) for now\n",
    "        df['target_placeholder'] = df['target']\n",
    "    return df[['placeholder_sentence', 'target_placeholder']]\n",
    "\n",
    "def preprocess_function(examples, tokenizer, max_length=128):\n",
    "    model_inputs = tokenizer(examples['placeholder_sentence'], max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['target_placeholder'], max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Set your paths and parameters\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "data_path = r\"E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\data\\entity_placeholders.csv\"\n",
    "output_dir = r\"E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\finetuned_placeholder_mt\"\n",
    "batch_size = 8\n",
    "num_train_epochs = 3\n",
    "max_length = 128\n",
    "\n",
    "# Load data\n",
    "df = load_data(data_path)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Preprocess\n",
    "tokenized_dataset = dataset.map(lambda x: preprocess_function(x, tokenizer, max_length), batched=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"no\",\n",
    "    logging_steps=100,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[\"none\"],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Fine-tuned model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a45c1f",
   "metadata": {},
   "source": [
    "## 6. Predict with Fine-tuned Model (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5924fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ptlpa\\anaconda3\\envs\\aisd_env1\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "100%|██████████| 91/91 [00:16<00:00,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\Data\\predictions\\finetuned_placeholder_mt\\fr_FR.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def save_jsonl(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def translate_sentences(sentences, model, tokenizer, batch_size=8):\n",
    "    translations = []\n",
    "    for i in tqdm(range(0, len(sentences), batch_size)):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        translated = model.generate(**inputs)\n",
    "        outputs = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        translations.extend(outputs)\n",
    "    return translations\n",
    "\n",
    "# Set paths\n",
    "val_path = r\"E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\Data\\references\\validation\\fr_FR.jsonl\"\n",
    "model_dir = r\"E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\entity_aware_mt\\finetuned_placeholder_mt\"\n",
    "output_dir = r\"E:\\AISD\\Term2\\NLP\\Project\\NER_SemEval_2025\\Data\\predictions\\finetuned_placeholder_mt\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"fr_FR.jsonl\")\n",
    "\n",
    "# Load validation data\n",
    "val_data = load_jsonl(val_path)\n",
    "# Determine which key holds the source text\n",
    "if 'placeholder_sentence' in val_data[0]:\n",
    "    src_texts = [ex['placeholder_sentence'] for ex in val_data]\n",
    "elif 'text' in val_data[0]:\n",
    "    src_texts = [ex['text'] for ex in val_data]\n",
    "elif 'source' in val_data[0]:\n",
    "    src_texts = [ex['source'] for ex in val_data]\n",
    "else:\n",
    "    raise ValueError(\"Could not find source text key in validation data.\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_dir)\n",
    "model = MarianMTModel.from_pretrained(model_dir)\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Translate\n",
    "translations = translate_sentences(src_texts, model, tokenizer)\n",
    "\n",
    "# Save predictions in same JSONL structure, add 'prediction' key\n",
    "for ex, pred in zip(val_data, translations):\n",
    "    ex['prediction'] = pred\n",
    "save_jsonl(val_data, output_path)\n",
    "print(f\"Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd263e",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "Evaluate translation quality using COMET.\n",
    "\n",
    "For Evaluation, Check eval.ipynb file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b472ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aisd_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
